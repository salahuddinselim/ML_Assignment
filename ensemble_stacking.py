# -*- coding: utf-8 -*-
"""ensemble_stacking.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WQXKiGBPM385egLRk1NJCq2jwZ76UULO
"""

import pandas as pd
import kagglehub
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier, StackingClassifier, VotingClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# 1. Download and Load Data
path = kagglehub.dataset_download("uciml/iris")
# Note: Kaggle path might vary slightly; usually it's path + "/Iris.csv"
df = pd.read_csv(f"{path}/Iris.csv")

# Preprocessing
X = df.drop(['Id', 'Species'], axis=1)
y = df['Species']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 2. Define Base Models for Stacking
base_models = [
    ('rf', RandomForestClassifier(n_estimators=10, random_state=42)),
    ('knn', KNeighborsClassifier(n_neighbors=3)),
    ('svc', SVC(probability=True))
]

# 3. Create the Stacking Classifier (Meta-model learns from base models)
stacking_model = StackingClassifier(
    estimators=base_models,
    final_estimator=LogisticRegression()
)

# 4. Create the Voting Classifier (Combines Stacking with a standalone model)
# We add a high-depth Decision Tree to the vote for variety
final_ensemble = VotingClassifier(
    estimators=[
        ('stacking', stacking_model),
        ('extra_rf', RandomForestClassifier(n_estimators=50))
    ],
    voting='soft' # 'soft' uses probabilities for better precision
)

# 5. Train and Evaluate
final_ensemble.fit(X_train, y_train)
y_pred = final_ensemble.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
print(f"Ensemble Model Accuracy: {accuracy * 100:.2f}%")